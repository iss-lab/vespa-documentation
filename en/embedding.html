---
# Copyright Vespa.ai. All rights reserved.
title: "Embedding"
redirect_from:
- /documentation/embedding.html
---

<p>
  A common technique is to map unstructured data - say, text or images -
  to points in an abstract vector space and then do computation in that space.
  For example, retrieve
  similar data by <a href="approximate-nn-hnsw.html">finding nearby points in the vector space</a>,
  or <a href="onnx.html">using the vectors as input to a neural net</a>.
  This mapping is referred to as <em>embedding</em>.
  Read more about embedding and embedding management in this
  <a href="https://blog.vespa.ai/tailoring-frozen-embeddings-with-vespa/">blog post</a>.
</p>

<p>Embedding vectors can be sent to Vespa in queries and writes:</p>
<img src="/assets/img/vespa-overview-embeddings-1.svg" alt="document- and query-embeddings"
  width="890px" height="auto"/>

<p>Alternatively, you can use the <code>embed</code> function to generate the embeddings inside Vespa
to reduce vector transfer cost and make clients simpler:
</p>
<img src="/assets/img/vespa-overview-embeddings-2.svg" alt="Vespa's embedding feature, creating embeddings from text"
  width="890px" height="auto"/>


<h2 id="configuring-embedders">Configuring embedders</h2>

<p>Embedders are <a href="jdisc/container-components.html">components</a> which must be configured in your
<a href="reference/services.html">services.xml</a>. Components are shared and can be used across schemas.</p>

<pre>{% highlight xml %}
<container id="default" version="1.0">
    <component id="hf-embedder" type="hugging-face-embedder">
        <transformer-model path="my-models/model.onnx"/>
        <tokenizer-model path="my-models/tokenizer.json"/>
    </component>
    ...
</container>
{% endhighlight %}</pre>

<p>You can <a href="https://javadoc.io/doc/com.yahoo.vespa/linguistics/latest/com/yahoo/language/process/Embedder.html">write your own</a>,
or use <a href="#provided-embedders">embedders provided in Vespa</a>.</p>


<h2 id="embedding-a-query-text">Embedding a query text</h2>

<p>Where you would otherwise supply a tensor in a query request,
you can (with an embedder configured) instead supply any text enclosed in <code>embed()</code>:</p>

<pre>
input.query(q)=<span class="pre-hilite">embed(myEmbedderId, "Hello%20world")</span>
</pre>

<p>Both single and double quotes are permitted, and if you have only configured a single embedder,
you can skip the embedder id argument and the quotes.</p>

<p>The text argument can be supplied by a referenced parameter instead, using the <code>@parameter</code> syntax:
<pre>{% highlight json %}
{
    "yql": "select * from doc where {targetHits:10}nearestNeighbor(embedding_field, query_embedding)",
    "text": "my text to embed",
    "input.query(query_embedding)": "embed(@text)",
}
{% endhighlight %}</pre>

</p>

<p>Remember that regardless of whether you are using embedders, input tensors
must always be <a href="reference/schema-reference.html#inputs">defined in the schema's rank-profile</a>.</p>


<h2 id="embedding-a-document-field">Embedding a document field</h2>

<p>Use the <code><a href="reference/indexing-language-reference.html#embed">embed</a></code> function
of the <a href="reference/indexing-language-reference.html#statement">indexing language</a>
to convert string into embeddings:

<pre>
schema doc {

    document doc {

        field title type string {
            indexing: summary | index
        }

    }

    field embeddings type tensor&lt;bfloat16&gt;(x[384]) {
        indexing {
            input title | <span class="pre-hilite">embed embedderId</span> | attribute | index
        }
    }

}
</pre>

<p>Notice that the embedding field is defined outside the <code>document</code> clause in the schema.
If you only have configured a single embedder you can skip the embedder id argument.</p>

<p>The input field can also be an array, where the output becomes a rank 2 tensor, see
<a href="https://blog.vespa.ai/semantic-search-with-multi-vector-indexing/">this blog post</a>:</p>

<pre>
schema doc {

    document doc {

        field chunks type array&lt;string&gt; {
            indexing: index | summary
        }

    }

    field embeddings type tensor&lt;bfloat16&gt;(p{},x[5]) {
        indexing: input chunks | <span class="pre-hilite">embed embedderId</span> | attribute | index
    }

}
</pre>



<h2 id="provided-embedders">Provided embedders</h2>

<p>Vespa provides several embedders as part of the platform.</p>

<h3 id="huggingface-embedder">Huggingface Embedder</h3>

<p>An embedder using any <a href="https://huggingface.co/docs/tokenizers/index">Huggingface tokenizer</a>,
including multilingual tokenizers,
to produce tokens which is then input to a supplied transformer model in <a href="https://onnx.ai/">ONNX</a> model format:</p>

<pre>{% highlight xml %}
<container id="default" version="1.0">
    <component id="hf-embedder" type="hugging-face-embedder">
        <transformer-model path="my-models/model.onnx"/>
        <tokenizer-model path="my-models/tokenizer.json"/>
    </component>
    ...
</container>
{% endhighlight %}</pre>

<p>The huggingface-embedder supports all
<a href="https://huggingface.co/docs/tokenizers/index">Huggingface tokenizer implementations</a>.</p>

<ul>
    <li>
        The <code>transformer-model</code> specifies the embedding model in <a href="https://onnx.ai/">ONNX</a> format.
        See <a href="onnx.html#onnx-export">exporting models to ONNX</a>,
        for how to export embedding models from Huggingface to be compatible with Vespa's <code>hugging-face-embedder</code>.
        Note that Vespa only supports models that are self-contained in a single ONNX file. If your model is split into multiple files,
        you need to merge them into a single file before using it with Vespa. For example using quantization. This also means that Vespa supports models up to 2GB in size.
    </li>
    <li>
        The <code>tokenizer-model</code> specifies the Huggingface <code>tokenizer.json</code> formatted file.
        See <a href="https://huggingface.co/transformers/v4.8.0/fast_tokenizers.html#loading-from-a-json-file">HF loading tokenizer from a json file.</a>
    </li>
</ul>

<p>Use <code>path</code> to supply the model files from the application package,
<code>url</code> to supply them from a remote server, or
<code>model-id</code> to use a
<a href="https://cloud.vespa.ai/en/model-hub#hugging-face-embedder">model supplied by Vespa Cloud</a>,
see <a href="reference/embedding-reference.html#model-config-reference">model config reference</a>.
</p>
<pre>{% highlight xml %}
<container id="default" version="1.0">
    <component id="e5" type="hugging-face-embedder">
        <transformer-model url="https://huggingface.co/intfloat/e5-small-v2/resolve/main/model.onnx"/>
        <tokenizer-model url="https://huggingface.co/intfloat/e5-small-v2/raw/main/tokenizer.json"/>
    </component>
    ...
</container>{% endhighlight %}</pre>

<p>See the <a href="reference/embedding-reference.html#huggingface-embedder-reference-config">reference</a>
for all configuration parameters.</p>

<h4 id="huggingface-embedder-models">Huggingface embedder models</h4>
<p>
    The following are examples of text embedding models that can be used with the hugging-face-embedder
    and their output <a href="tensor-user-guide.html">tensor</a> dimensionality.
    The resulting <a href="reference/tensor.html#tensor-type-spec">tensor type</a> can be <code>float</code>,
    <code>bfloat16</code> or using binarized quantization into <code>int8</code>.
    See blog post <a href="https://blog.vespa.ai/combining-matryoshka-with-binary-quantization-using-embedder/">Combining matryoshka with binary-quantization</a>
    for more examples on using the Huggingface embedder with binary quantization.
</p>

<p>The following models use <code>pooling-strategy</code> <code>mean</code>,
   which is the default <a href="reference/embedding-reference.html#huggingface-embedder-reference-config">pooling-strategy</a>:</p>
<ul>
    <li><a href="https://huggingface.co/intfloat/e5-small-v2">intfloat/e5-small-v2</a> produces <code>tensor&lt;float&gt;(x[384])</code></li>
    <li><a href="https://huggingface.co/intfloat/e5-base-v2">intfloat/e5-base-v2</a> produces <code>tensor&lt;float&gt;(x[768])</code></li>
    <li><a href="https://huggingface.co/intfloat/e5-large-v2">intfloat/e5-large-v2</a> produces <code>tensor&lt;float&gt;(x[1024])</code></li>
    <li><a href="https://huggingface.co/intfloat/multilingual-e5-base">intfloat/multilingual-e5-base</a> produces <code>tensor&lt;float&gt;(x[768])</code></li>
</ul>
<p>
  The following models are useful for binarization and Matryoshka dimensionality flexibility where only the first k 
  dimensions are retained.
  <a href="https://blog.vespa.ai/combining-matryoshka-with-binary-quantization-using-embedder/">
    Matryoshka ü§ù Binary vectors: Slash vector search costs with Vespa</a> is a great read on this subject.
  When enabling binarization with <code>int8</code> use <a href="reference/schema-reference.html#hamming">distance-metric hamming</a>:
</p>
<ul>
    <li><a href="https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1">mxbai-embed-large-v1</a> produces <code>tensor&lt;float&gt;(x[1024])</code>. This model
    is also useful for binarization which can be triggered by using destination <code>tensor&lt;int8&gt;(x[128])</code>.
    Use <code>pooling-strategy</code> <code>cls</code> and <code>normalize</code> <code>true</code>.</li>
    <li><a href="https://huggingface.co/nomic-ai/nomic-embed-text-v1.5">nomic-embed-text-v1.5</a> produces <code>tensor&lt;float&gt;(x[768])</code>. This model
      is also useful for binarization which can be triggered by using destination <code>tensor&lt;int8&gt;(x[96])</code>.  Use <code>normalize</code> <code>true</code>.</li>
</ul>

<p>Snowflake arctic model series:</p>
<ul>
  <li><a href="https://huggingface.co/Snowflake/snowflake-arctic-embed-xs">snowflake-arctic-embed-xs</a> produces <code>tensor&lt;float&gt;(x[384])</code>.
    Use <code>pooling-strategy</code> <code>cls</code> and <code>normalize</code> <code>true</code>.</li>
    <li><a href="https://huggingface.co/Snowflake/snowflake-arctic-embed-m">snowflake-arctic-embed-m</a> produces <code>tensor&lt;float&gt;(x[768])</code>.
      Use <code>pooling-strategy</code> <code>cls</code> and <code>normalize</code> <code>true</code>.</li>
</ul>

<p>All of these example text embedding models can be used in combination with Vespa's
<a href="nearest-neighbor-search.html">nearest neighbor search</a>
using the appropriate <a href="reference/schema-reference.html#distance-metric">distance-metric</a>. Notice that in order
to use the <a href="https://docs.vespa.ai/en/reference/schema-reference.html#prenormalized-angular">distance-metric: prenormalized-angular</a>, the
<code>normalize</code> configuration must be set to <code>true</code>.</p>

<p>Check the <a href="https://huggingface.co/blog/mteb">Massive Text Embedding Benchmark</a> (MTEB) benchmark and
<a href="https://huggingface.co/spaces/mteb/leaderboard">MTEB leaderboard</a>
for help with choosing an embedding model.</p>


<h3 id="bert-embedder">Bert embedder</h3>

<p>An embedder using the <a href="reference/embedding-reference.html#wordpiece-embedder">WordPiece</a> embedder to produce tokens
which are then input to a supplied <a href="https://onnx.ai/">ONNX</a> model on the form expected by a BERT base model:</p>

<pre>{% highlight xml %}
<container version="1.0">
  <component id="myBert" type="bert-embedder">
    <transformer-model url="https://huggingface.co/intfloat/e5-small-v2/resolve/main/model.onnx"/>
    <tokenizer-vocab url="https://huggingface.co/intfloat/e5-small-v2/raw/main/vocab.txt"/>
    <max-tokens>128</max-tokens>
  </component>
</container>
{% endhighlight %}</pre>
<ul>
    <li>
        The <code>transformer-model</code> specifies the embedding model in <a href="https://onnx.ai/">ONNX</a> format.
        See <a href="onnx.html#onnx-export">exporting models to ONNX</a>,
        for how to export embedding models from Huggingface to compatible <a href="https://onnx.ai/">ONNX</a> format.
    </li>
    <li>
        The <code>tokenizer-vocab</code> specifies the Huggingface <code>vocab.txt</code> file, with one valid token per line.
        Note that the Bert embedder does not support the <code>tokenizer.json</code> formatted tokenizer configuration files.
        This means that tokenization settings like max tokens should be set explicitly.
    </li>
</ul>

<p>The Bert embedder is limited to English (<a href="reference/embedding-reference.html#wordpiece-embedder">WordPiece</a>) and
BERT-styled transformer models with three model inputs
(<em>input_ids, attention_mask, token_type_ids</em>).
Prefer using the <a href="#huggingface-embedder">Huggingface Embedder</a> instead of the Bert embedder.</p>

<p>See <a href="reference/embedding-reference.html#bert-embedder-reference-config">configuration reference</a> for all configuration options.</p>


<h3 id="colbert-embedder">ColBERT embedder</h3>

<p>An embedder supporting <a href="https://github.com/stanford-futuredata/ColBERT">ColBERT</a> models. The
ColBERT embedder maps text to <i>token</i> embeddings, representing a text as multiple
contextualized embeddings. This produces better quality than reducing all tokens into a single vector.</p>

<p>Read more about ColBERT and the ColBERT embedder in blog post form
  <a href="https://blog.vespa.ai/announcing-colbert-embedder-in-vespa/">Announcing the Vespa ColBERT embedder</a>
  and <a href="https://blog.vespa.ai/announcing-long-context-colbert-in-vespa/">Announcing Vespa Long-Context ColBERT</a>.</p>

<pre>{% highlight xml %}
<container version="1.0">
    <component id="colbert" type="colbert-embedder">
      <transformer-model url="https://huggingface.co/colbert-ir/colbertv2.0/resolve/main/model.onnx"/>
      <tokenizer-model url="https://huggingface.co/colbert-ir/colbertv2.0/raw/main/tokenizer.json"/>
      <max-query-tokens>32</max-query-tokens>
      <max-document-tokens>128</max-document-tokens>
    </component>
</container>
{% endhighlight %}</pre>

<ul>
    <li>
        The <code>transformer-model</code> specifies the ColBERT embedding model in <a href="https://onnx.ai/">ONNX</a> format.
        See <a href="onnx.html#onnx-export">exporting models to ONNX</a>,
        for how to export embedding models from Huggingface to compatible <a href="https://onnx.ai/">ONNX</a> format.
        The <a href="https://huggingface.co/vespa-engine/col-minilm">vespa-engine/col-minilm</a> page on the HF
        model hub has a detailed example of how to export a colbert checkpoint to ONNX format for accelerated inference.
    </li>
    <li>
        The <code>tokenizer-model</code> specifies the Huggingface <code>tokenizer.json</code> formatted file.
        See <a href="https://huggingface.co/transformers/v4.8.0/fast_tokenizers.html#loading-from-a-json-file"> HF loading tokenizer from a json file.</a>
    </li>
    <li>
        The <code>max-query-tokens</code> controls the maximum number of query text tokens that are represented as vectors and
        similarly <code>max-document-tokens</code> controls the document side. These parameters
        can be used to control resource usage.
    </li>
</ul>
<p>See <a href="reference/embedding-reference.html#colbert-embedder-reference-config">configuration reference</a> for all
configuration options and defaults.</p>

<p>The ColBERT token embeddings are represented as a
<a href="tensor-user-guide.html#tensor-concepts">mixed tensor</a>: <code>tensor&lt;float&gt;(token{}, x[dim])</code> where
<code>dim</code> is the vector dimensionality of the contextualized token embeddings.
The <a href="https://huggingface.co/colbert-ir/colbertv2.0">colbert model checkpoint</a> on Hugging Face hub
uses 128 dimensions.</p>

<p>The embedder destination tensor is defined in the <a href="schemas.html">schema</a>, and
depending on the target <a href="reference/tensor.html#tensor-type-spec">tensor cell precision</a> definition
the embedder can compress the representation:

    If the target tensor cell type is <code>int8</code>, the ColBERT embedder compress the token embeddings with binarization for
    the document to reduce storage to 1-bit per value, reducing the token embedding storage footprint
    by 32x compared to using float. The <i>query</i> representation is not compressed with binarization.
    The following demonstrates two ways to use the ColBERT embedder in
    the document schema to <a href="#embedding-a-document-field">embed a document field</a>.
</p>

<pre>
schema doc {
  document doc {
    field text type string {..}
  }
  field colbert_tokens type tensor&lt;float&gt;(token{}, x[128]) {
    indexing: input text | embed colbert | attribute
  }
  field colbert_tokens_compressed type tensor&lt;int8&gt;(token{}, x[16]) {
    indexing: input text | embed colbert | attribute
  }
}
</pre>

<p>The first field <code>colbert_tokens</code> store the original representation as the tensor destination
cell type is float. The second field, the <code>colbert_tokens_compressed</code> tensor is compressed.
When using <code>int8</code> tensor cell precision, one
should divide the original vector size by 8 (128/8 = 16).</p>

<p>You can also use <code>bfloat16</code> instead of <code>float</code> to reduce storage by 2x compared to <code>float</code>.</p>
<pre>
field colbert_tokens type tensor&lt;bfloat16&gt;(token{}, x[128]) {
  indexing: input text | embed colbert | attribute
}
</pre>

<p>You can also use the ColBERT embedder with an array of strings (representing chunks):</p>
<pre>
schema doc {
  document doc {
    field chunks type array&lt;string&gt; {..}
  }
  field colbert_tokens_compressed type tensor&lt;int8&gt;(chunk{}, token{}, x[16]) {
    indexing: input text | embed colbert chunk | attribute
  }
}
</pre>

<p>Here, we need a second mapped dimension in the target tensor, and a second argument to embed,
telling the ColBERT embedder the name of the tensor dimension to use for the chunks.</p>

<p>Notice that the examples above did not specify the <code>index</code> function for creating a
<a href="approximate-nn-hnsw.html">HNSW</a> index.
The colbert representation is intended to be used as a ranking model,
and not for retrieval with Vespa's nearestNeighbor query operator,
where you can e.g. use a document level vector and/or lexical matching.</p>

<p>To reduce memory footprint, use <a href="attributes.html#paged-attributes">paged attributes</a>.</p>

<h4 id="colbert-ranking">ColBERT ranking</h4>

See sample-applications for how to use ColBERT in ranking with variants of the MaxSim similarity operator
expressed using Vespa tensor computation expressions. See:

<a href="https://github.com/vespa-engine/sample-apps/tree/master/colbert">colbert</a> and
 <a href="https://github.com/vespa-engine/sample-apps/tree/master/colbert-long">colbert-long</a>.

<h3 id="splade-embedder">SPLADE embedder</h3>

<p>An embedder supporting <a href="https://github.com/naver/splade">SPLADE</a> models. The
SPLADE embedder maps text to mapped tensor, representing a text as a sparse vector of unique tokens and their weights.</p>


<pre>{% highlight xml %}
<container version="1.0">
    <component id="splade" type="splade-embedder">
      <transformer-model path="models/splade_model.onnx"/>
      <tokenizer-model path="models/tokenizer.json"/>
    </component>
</container>
{% endhighlight %}</pre>

<ul>
    <li>
        The <code>transformer-model</code> specifies the SPLADE embedding model in <a href="https://onnx.ai/">ONNX</a> format.
        See <a href="onnx.html#onnx-export">exporting models to ONNX</a>,
        for how to export embedding models from Huggingface to compatible <a href="https://onnx.ai/">ONNX</a> format.
    </li>
    <li>
        The <code>tokenizer-model</code> specifies the Huggingface <code>tokenizer.json</code> formatted file.
        See <a href="https://huggingface.co/transformers/v4.8.0/fast_tokenizers.html#loading-from-a-json-file"> HF loading tokenizer from a json file.</a>
    </li>
</ul>
<p>See <a href="reference/embedding-reference.html#splade-embedder-reference-config">configuration reference</a> for all
configuration options and defaults.</p>

<p>The splade token weights are represented as a
  <a href="tensor-user-guide.html#tensor-concepts">mapped tensor</a>: <code>tensor&lt;float&gt;(token{})</code>. </p>
  <p>The embedder destination tensor is defined in the <a href="schemas.html">schema</a>.
  The following demonstrates how to use the SPLADE embedder in the document schema to <a href="#embedding-a-document-field">embed a document field</a>.</p>
  </p>

  <pre>
  schema doc {
    document doc {
      field text type string {..}
    }
    field splade_tokens type tensor&lt;float&gt;(token{}) {
      indexing: input text | embed splade | attribute
    }
  }
  </pre>

<p>You can also use the SPLADE embedder with an array of strings (representing chunks). Here, also
  using lower tensor cell precision <code>bfloat16</code>:</p>
<pre>
schema doc {
  document doc {
    field chunks type array&lt;string&gt; {..}
  }
  field splade_tokens type tensor&lt;bfloat16&gt;(chunk{}, token{}) {
    indexing: input text | embed splade chunk | attribute
  }
}
</pre>

<p>Here, we need a second mapped dimension in the target tensor, and a second argument to embed,
telling the splade embedder the name of the tensor dimension to use for the chunks.</p>

<p>To reduce memory footprint, use <a href="attributes.html#paged-attributes">paged attributes</a>.</p>

<h4 id="splade-ranking">SPLADE ranking</h4>
See the splade <a href="https://github.com/vespa-engine/sample-apps/tree/master/splade">splade</a> sample application for how to use SPLADE in ranking. including
also how to use the SPLADE embedder with an array of strings (representing chunks).

<h2 id="embedder-performance">Embedder performance</h2>

<p>Embedding inference can be resource intensive for larger embedding models. Factors that impacts performance:</p>

<ul>
  <li>The embedding model parameters. Larger models are more expensive to evaluate than smaller models.</li>
  <li>The sequence input length. Transformer models scales quadratic with input length. Since queries
    are typically shorter than documents, embedding queries is less computationally intensive than embedding documents.
  </li>
  <li>
    The number of inputs to the <code>embed</code> call. When encoding arrays, consider how many inputs a single document can have.
    For CPU inference, increasing <a href="reference/document-v1-api-reference.html#timeout">feed timeout</a> settings
    might be required when documents have many <code>embed</code>inputs.
  </li>
</ul>
<p>Using <a href="reference/embedding-reference.html#embedder-onnx-reference-config">GPU</a>, especially for longer sequence lengths (documents),
can dramatically improve performance and reduce cost.
See the blog post on <a href="https://blog.vespa.ai/gpu-accelerated-ml-inference-in-vespa-cloud/">GPU-accelerated ML inference in Vespa Cloud</a>.
With GPU-accelerated instances, using fp16 models instead of fp32 can increase throughput by as much as 3x compared to fp32.</p>


<h2 id="metrics">Metrics</h2>

<p>Vespa's built-in embedders emits metrics for computation time and token sequence length.
These metrics are prefixed with <code>embedder.</code>
and listed in the <a href="reference/container-metrics-reference.html">Container Metrics</a> reference documentation.
Third-party embedder implementations may inject the <code>ai.vespa.embedding.Embedder.Runtime</code> component to easily
emit the same predefined metrics, although emitting custom metrics is perfectly fine.</p>


<h2 id="sample-applications">Sample applications</h2>

<p>These sample applications use embedders:</p>
<ul>
    <li><a href="https://github.com/vespa-engine/sample-apps/tree/master/simple-semantic-search">
        simple-semantic-search</a></li>
    <li><a href="https://github.com/vespa-engine/sample-apps/tree/master/commerce-product-ranking">commerce-product-ranking</a> -
        demonstrates using multiple embedders</li>
    <li><a href="https://github.com/vespa-engine/sample-apps/tree/master/multi-vector-indexing">multi-vector-indexing</a>
    demonstrates how to use embedders with multiple document field inputs</li>
    <li><a href="https://github.com/vespa-engine/sample-apps/tree/master/colbert">colbert</a>
      demonstrates how to use the colbert-embedder </li>
      <li><a href="https://github.com/vespa-engine/sample-apps/tree/master/colbert-long">colbert-long</a>
        demonstrates how to use the colbert-embedder with long contexts (array input) </li>
      <li><a href="https://github.com/vespa-engine/sample-apps/tree/master/splade">splade</a> demonstrates
      how to use the splade-embedder.</li>
</ul>

<h2 id="tricks-and-tips">Tricks and tips</h2>

<p>Various tricks that are useful with embedders.</p>

<h3 id="adding a fixed string to a query text">Adding a fixed string to a query text</h3>

<p>If you need to add a standard wrapper or a prefix instruction around the input text you want to embed
    use parameter substitution to supply the text, as in <code>embed(myEmbedderId, @text)</code>,
    and let the parameter (<code>text</code> here) be defined in a <a href="query-profiles.html">query profile</a>,
    which in turn uses <a href="query-profiles.html#value-substitution">value substitution</a>
    to place another query request supplied text value within it. The following is a concrete example
  where queries should have a prefix instruction before embedded to vector representation. The following
  defines a <code>text</code> input field to <code>search/query-profiles/default.xml</code>:</p>

<pre>
  &lt;query-profile id=&quot;default&quot;&gt;
      &lt;field name=&quot;text&quot;&gt;Represent this sentence for searching relevant passages: %{user_query}&lt;/field&gt;
  &lt;/query-profile&gt;
</pre>

<p>Then at query request time, we can pass <code>user_query</code> as a request parameter, this parameter is then used to produce
the <code>text</code> value which then is embedded.</p>
<pre>
  {
    "yql": "select * from doc where userQuery() or ({targetHits: 100}nearestNeighbor(embedding, e))",
    "input.query(e)": "embed(mxai, @text)",
    "user_query": "space contains many suns"
}
</pre>

The text that is embedded by the embedder is then:
<em>Represent this sentence for searching relevant passages: space contains many suns</em>.


<h3 id="concatenating-input-fields">Concatenating input fields</h3>

<p>You can concatenate values in indexing, using ".", and handle missing field values, using
    <a href="/en/indexing.html#choice-example">choice</a>
    to produce a single input for an embedder:
</p>

<pre>
schema doc {

    document doc {

        field title type string {
            indexing: summary | index
        }

        field body type string {
            indexing: summary | index
        }

    }

    field embeddings type tensor&lt;bfloat16&gt;(x[384]) {
        indexing {
            (input title || "") . " " . (input body || "") | <span class="pre-hilite">embed embedderId</span> | attribute | index
        }
        index: hnsw
    }

}
</pre>

<p>You can also use concatenation to add a fixed preamble to the string to embed.</p>


<h3 id="combining-with-foreach">Combining with foreach</h3>

<p>The indexing expression can also use <code>for_each</code> and include other document fields.
    For example the <em>E5</em> family of embedding models uses instructions along with the input. The following
    expression prefixes the input with <em>passage: </em> followed by a concatenation of the title and a text chunk.</p>

<pre>
schema doc {

    document doc {

        field title type string {
            indexing: summary | index
        }

        field chunks type array&lt;string&gt; {
            indexing: index | summary
        }

    }
    field embedding type tensor&lt;bfloat16&gt;(p{}, x[384]) {
        indexing {
            input chunks |
                for_each {
                    "passage: " . (input title || "") . " " . ( _ || "")
                } | embed e5 | attribute | index
        }
        attribute {
            distance-metric: prenormalized-angular
        }
    }
}
</pre>
<p>See <a href="/en/indexing.html#execution-value-example">Indexing language execution value</a>for details.</p>



<h2 id="troubleshooting">Troubleshooting</h2>
<p>
  If models fail to download, it will cause the Vespa Container to not start with
  <code>RuntimeException: Not able to create config builder for payload</code> -
  see <a href="/en/jdisc/container-components.html#component-load">example</a>.
</p>
