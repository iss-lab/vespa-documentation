---
# Copyright Vespa.ai. All rights reserved.
title: "Ranking Expressions"
redirect_from:
- /documentation/reference/ranking-expressions.html
---

<p>
  This is a complete reference to the <em>ranking expressions</em>
  used to configure application specific ranking functions.
  For examples and an overview of how to use ranking expressions,
  see the <a href="../ranking.html">ranking overview</a>.
</p>
<p>
  Ranking expressions are written in a simple language similar to ordinary functional notation.
  The atoms in ranking expressions are <em>rank features</em> and <em>constants</em>.
  These atoms can be combined by <em>arithmetic operations</em> and other
  <em>built-in functions</em> over scalars and tensor.
</p>
<table class="table">
<thead>
</thead><tbody>
<tr><th style="white-space:nowrap;">Rank Features</th>
<td>
  <p id="rank-features">
    A rank feature is a named value calculated or looked up by vespa for
    each query/document combination.  See the
    <a href="rank-features.html">rank feature reference</a>
    for a list of all the rank features available to ranking expressions.
  </p>
</td>
</tr>
<tr><th>Constants</th>
  <td>
    <p id="constants">
    A constant is either a floating point number, a boolean (true/false) or a quoted string.
    Since ranking expressions can only work on scalars and tensors, strings and booleans are
    immediately converted to scalars - true becomes 1.0, false 0.0 and a string its hash value.
    This means that <b>strings can only be used for equality comparisons</b>, other purposes
    such as parametrizing the key to slice out of a tensor will not work correctly.
    </p>
  </td>
</tr>
</tbody>
</table>



<h2 id="arithmetic-operations">Arithmetic operations</h2>

<p>Basic mathematical operations are expressed in in-fix notation:</p>

<pre>
a + b * c
</pre>

<p>Arithmetic operations work on any tensor in addition to scalars, and are a short form
of joining the tensors with the arithmetic operation used to join the cells.
For example <code>tensorA * tensorB</code> is the same as <code>join(tensorA, tensorB, f(a,b)(a * b))</code>.</p>

<p>All arithmetic operators in order of decreasing precedence:</p>

<table class="table">
<thead>
<tr>
<th>Arithmetic operator</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr><td>^</td> <td>Power</td></tr>
<tr><td>%</td> <td>Modulo</td></tr>
<tr><td>/</td> <td>Division</td></tr>
<tr><td>*</td> <td>Multiplication</td></tr>
<tr><td>-</td> <td>Subtraction</td></tr>
<tr><td>+</td> <td>Addition</td></tr>
<tr><td>&&</td> <td>And: 1 if both arguments are non-zero, 0 otherwise.</td></tr>
<tr><td>||</td> <td>Or: 1 if either argument is non-zero, 0 otherwise.</td></tr>
</tbody>
</table>



<h2 id="mathematical-scalar-functions">Mathematical scalar functions</h2>
<table class="table">
  <thead>
  <tr>
    <th>Function</th>
    <th>Description</th>
  </tr>
  </thead>
  <tbody>
  <tr><th>acos(<em>x</em>)</th> <td><p id="acos-x">Inverse cosine of <em>x</em></p></td></tr>
    <tr><th>asin(<em>x</em>)</th> <td><p id="asin-x">Inverse sine of <em>x</em></p></td></tr>
    <tr><th>atan(<em>x</em>)</th> <td><p id="atan-x">Inverse tangent of <em>x</em></p></td></tr>
    <tr><th>atan2(<em>y</em>, <em>x</em>)</th>
      <td><p id="atan2-x">Inverse tangent of <em>y / x</em>,
        using signs of both arguments to determine correct quadrant.</p></td>
    </tr>
    <tr><th>bit(<em>x</em>, <em>y</em>)</th>
      <td><p id="bit-x-y">Returns value of bit <em>y</em> in value <em>x</em> (for int8 values)</p></td>
    </tr>
    <tr><th>ceil(<em>x</em>)</th> <td><p id="ceil-x">Lowest integral value not less than <em>x</em></p></td></tr>
    <tr><th>cos(<em>x</em>)</th> <td><p id="cos-x">Cosine of <em>x</em></p></td></tr>
    <tr><th>cosh(<em>x</em>)</th> <td><p id="cosh-x">Hyperbolic cosine of <em>x</em></p></td></tr>
    <tr><th>elu(<em>x</em>)</th> <td><p id="elu-x">The Exponential Linear Unit activation function for value <em>x</em></p></td></tr>
    <tr><th>erf(<em>x</em>)</th> <td><p id="erf-x">The Gauss error function for value <em>x</em></p></td></tr>
    <tr><th>exp(<em>x</em>)</th> <td><p id="exp-x">Base-e exponential function.</p></td></tr>
    <tr><th>fabs(<em>x</em>)</th> <td><p id="fabs-x">Absolute value of (floating-point) number <em>x</em></p></td></tr>
    <tr><th>floor(<em>x</em>)</th> <td><p id="floor-x">Largest integral value not greater than <em>x</em></p></td></tr>
    <tr><th>fmod(<em>x</em>, <em>y</em>)</th> <td><p id="fmod-x">Remainder of <em>x / y</em></p></td></tr>
    <tr><th>isNan(<em>x</em>)</th> <td><p id="isnan-x">Returns 1.0 if <em>x</em> is NaN, 0.0 otherwise</p></td></tr>
    <tr><th>ldexp(<em>x</em>, <em>exp</em>)</th> <td><p id="ldexp-x">Multiply <em>x</em> by 2 to the power of <em>exp</em></p></td></tr>
    <tr><th>log(<em>x</em>)</th> <td><p id="log-x">Base-e logarithm of <em>x</em></p></td></tr>
    <tr><th>log10(<em>x</em>)</th> <td><p id="log10-x">Base-10 logarithm of <em>x</em></p></td></tr>
    <tr><th>max(<em>x</em>, <em>y</em>)</th> <td><p id="max-x">Larger of <em>x</em> and <em>y</em></p></td></tr>
    <tr><th>min(<em>x</em>, <em>y</em>)</th> <td><p id="min-x">Smaller of <em>x</em> and <em>y</em></p></td></tr>
    <tr><th>pow(<em>x</em>, <em>y</em>)</th> <td><p id="pow-x">Return <em>x</em> raised to the power of <em>y</em></p></td></tr>
    <tr><th>relu(<em>x</em>)</th> <td><p id="relu-x">The Rectified Linear Unit activation function for value <em>x</em></p></td></tr>
    <tr><th>sigmoid(<em>x</em>)</th> <td><p id="sigmoid-x">The sigmoid (logistic) activation function for value <em>x</em></p></td></tr>
    <tr><th>sin(<em>x</em>)</th> <td><p id="sin-xx">Sine of <em>x</em></p></td></tr>
    <tr><th>sinh(<em>x</em>)</th> <td><p id="sinh-x">Hyperbolic sine of <em>x</em></p></td></tr>
    <tr><th>sqrt(<em>x</em>)</th> <td><p id="sqrt-x">Square root of <em>x</em></p></td></tr>
    <tr><th>tan(<em>x</em>)</th> <td><p id="tan-x">Tangent of <em>x</em></p></td></tr>
    <tr><th>tanh(<em>x</em>)</th> <td><p id="tanh-x">Hyperbolic tangent of <em>x</em></p></td></tr>
    <tr><th>hamming(<em>x</em>, <em>y</em>)</th>
      <td><p id="hamming-x-y">Hamming (bit-wise) distance between <em>x</em> and <em>y</em> (considered as 8-bit integers).</p></td>
    </tr>
  </tbody>
</table>
<p><code>x</code> and <code>y</code> may be any ranking expression.</p>



<h2 id="the-if-function">The if function</h2>
<p>
The <code>if</code> function chooses between two sub-expressions based on the truth value of a condition.
</p>
<pre>
if (expression1 <span class="pre-hilite">operator</span> expression2, trueExpression, falseExpression)
</pre>
<p>
If the condition given in the first argument is true,
the expression in argument 2 is used, otherwise argument 3.
The four expressions may be any ranking expression.
Conditional operators in ranking expression if functions:
</p>
<table class="table">
  <thead>
  <tr>
    <th>Boolean operator</th>
    <th>Description</th>
  </tr>
  </thead>
  <tbody>
    <tr><td>&lt;=</td> <td>Less than or equal</td></tr>
    <tr><td>&lt;</td> <td>Less than</td></tr>
    <tr><td>==</td> <td>Equal</td></tr>
    <tr><td>~=</td> <td>Approximately equal</td></tr>
    <tr><td>&gt;=</td> <td>Greater than or equal</td></tr>
    <tr><td>&gt;</td> <td>Greater than</td></tr>
  </tbody>
</table>
<p>The <code>in</code> membership operator uses a slightly modified if-syntax:</p>
<pre>
if (expression1 <span class="pre-hilite">in</span> [expression2, expression3, ..., expressionN], trueExpression, falseExpression)
</pre>
<p>
If expression1 is equal to either of expression1 through expressionN,
then trueExpression is used, otherwise falseExpression.
</p>



<h2 id="the-foreach-function">The foreach function</h2>
<p>
The foreach function is not really part of the expression language but implemented as a
<a href="rank-features.html#foreach(dimension,variable,feature,condition,operation)">rank feature</a>.
</p>



<h2 id="tensor-functions">Tensor functions</h2>
<p>
The following set of tensors functions are available to use in ranking expressions.
The functions are grouped in primitive functions
and convenience functions that can be implemented in terms of the primitive ones.
</p>


<h3 id="primitive-functions">Primitive functions</h3>
<table class="table">
  <thead>
  <tr>
    <th>Function</th>
    <th>Description</th>
  </tr>
  </thead>
<tbody>
<tr>
    <th>map(<br/>&nbsp;&nbsp;tensor,<br/>&nbsp;&nbsp;f(x)(expr)<br/>)</th>
    <td>
        <p id="map">
        Returns a new tensor with the lambda function defined in <code>f(x)(expr)</code> applied to each cell.
        </p><p>
        Arguments:
        </p>
        <ul>
          <li><code>tensor</code>: a tensor expression. For example <code>attribute(tensor_field)</code></li>
          <li><code>f(x)(expr)</code>: a <a href="#lambda">lambda function</a> with one argument.</li>
        </ul>
        Returns a new tensor where the expression in the lambda function is
        evaluated in each cell in <code>tensor</code>. Examples:
<pre>
map(t, f(x)(x*x))
</pre>
<a href="https://docs.vespa.ai/playground/#N4KABGBEBmkFxgNrgmUrWQPYAd5QFNIAaFDSPBdDTAO30gEcBXAgJwE8AKAFwEoSZGpCIJIPArQDOWNgB5oAGywBDHgD4uAD0QAWALp84iAAzEAjMQBMxAMz7IQiAF8hz0hmrlcDIh+GUaE50DAC2KjgA+gRaKqE4in7BECJhEVws7Nz8xGDQ2nzaAHpWfALBrhiVYPogzkA">
  playground example map
</a>
    </td>
</tr>
<tr>
  <th>map_subspaces(<br/>&nbsp;&nbsp;tensor,<br/>&nbsp;&nbsp;f(x)(expr)<br/>)</th>
  <td>
    <p id="map-subspaces">
      Returns a new tensor with the lambda function defined in <code>f(x)(expr)</code> applied to each dense subspace.
    </p><p>
      Arguments:
    </p>
    <ul>
      <li><code>tensor</code>: a tensor expression. For example <code>attribute(tensor_field)</code></li>
      <li><code>f(x)(expr)</code>: a <a href="#lambda">lambda function</a> with one argument.</li>
    </ul>
    Returns a new tensor where the lambda function is evaluated for
    each dense subspace in <code>tensor</code>. This is an advanced
    feature that enables using dense <a href="#tensor">tensor
    generator</a> expressions to transform mixed tensors. Examples:
    <pre>
      map_subspaces(tensor(x{},y[3]):{a:[1,2,3]},f(d)(tensor(z[2])(d{y:(z)}+d{y:(z+1)})))
    </pre>
    <a href="https://docs.vespa.ai/playground/#N4KABGBEBmkFxgNrgmUrWQPYAd5QFNIAaFDSPBdDTAO30gEcBXAgJwE8AKAFwEoSZGpCIJIPArQDOWNgB4AlrR4AOAHxcAHsAC+xDogBMAXT5xgAQziIAjAFZiAWhsA2YzshCIOoXqHVyXAYiUhooSjQvOgZmWhwLAGMAawB9ACMFHikUgHdMgAsUgFsLHBSpZjSpeISCKUEwiBEGErKKqpq6rhZ2bn5iaC4AEz5eSRl5JVUNA1dTLgyeYeAOOC4OAHoVPj0AdkcOAFJtvlPPMJ8MS7BjEB0gA">
      playground example for map_subspaces
    </a>
  </td>
</tr>
<tr>
    <th>reduce(<br/>&nbsp;&nbsp;tensor,<br/>&nbsp;&nbsp;aggregator,<br/>&nbsp;&nbsp;dim1,<br/>&nbsp;&nbsp;dim2,<br/>&nbsp;&nbsp;...<br/>)</th>
    <td>
        <p id="reduce">
        Returns a new tensor with the <code>aggregator</code> applied across dimensions dim1, dim2, etc.
        If no dimensions are specified, reduce over all dimensions.
        </p><p>
        Arguments:
        </p>
        <ul>
          <li><code>tensor</code>: a tensor expression.</li>
          <li><code>aggregator</code>: the aggregator to use. See below.</li>
          <li><code>dim1, dim2, ...</code>: the dimensions to reduce over. Optional.</li>
        </ul>
        <p>
        Returns a new tensor with the aggregator applied across dimensions
        <code>dim1</code>, <code>dim2</code>, etc.
        If no dimensions are specified, reduce over all dimensions.
        </p><p>
        Available aggregators are:
        </p>
        <ul>
          <li><code>avg</code>: arithmetic mean</li>
          <li><code>count</code>: number of elements</li>
          <li><code>max</code>: maximum value</li>
          <li><code>median</code>: median value</li>
          <li><code>min</code>: minimum value</li>
          <li><code>prod</code>: product of all values</li>
          <li><code>sum</code>: sum of all values</li>
        </ul>
        Examples:
<pre>
reduce(t, sum)         # Sum all values in tensor
reduce(t, count, x)    # Count number of cells along dimension x
</pre>
<a href="https://docs.vespa.ai/playground/#N4KABGBEBmkFxgNrgmUrWQPYAd5QFNIAaFDSPBdDTAO30gEcBXAgJwE8AKAFwEoSZGpCIJIPArQDOWNgB5oAGywBDHgD4uAD0QAWALp84iAAzEAjMQBMxAMz7IQiAF8hz0hmrlcDIh+GUaE50DGwEACbMAMYEAPpSzAC2sQRaKok4in7BECKhEdEEXCzs3PzEYAmJAsGuGHVg+iDOQA">
  playground example reduce
</a>
    </td>
</tr>
<tr>
    <th>join(<br/>&nbsp;&nbsp;tensor1,<br/>&nbsp;&nbsp;tensor2,<br/>&nbsp;&nbsp;f(x,y)(expr)<br/>)</th>
    <td>
        <p id="join">
        Returns a new tensor constructed from the <em>natural join</em> between <code>tensor1</code> and <code>tensor2</code>,
        with the resulting cells having the value as calculated from <code>f(x,y)(expr)</code>, where <code>x</code>
        is the cell value from <code>tensor1</code> and <code>y</code> from <code>tensor2</code>.
        </p><p>
        Arguments:
        </p>
        <ul>
          <li><code>tensor1</code>: a tensor expression.</li>
          <li><code>tensor2</code>: a tensor expression.</li>
          <li><code>f(x,y)(expr)</code>: a <a href="#lambda">lambda function</a> with two arguments.</li>
        </ul>
        <p>
        Returns a new tensor constructed from the <em>natural join</em>
        between <code>tensor1</code> and <code>tensor2</code>,
        with the resulting cells having the value as calculated from <code>f(x,y)(expr)</code>,
        where <code>x</code> is the cell value from <code>tensor1</code> and
        <code>y</code> from <code>tensor2</code>.
        </p><p>
        Formally, the result of the <code>join</code> is a new tensor with dimensions
        the union of dimension between <code>tensor1</code> and <code>tensor2</code>.
        The cells are the set of all combinations of cells that have equal values
        on their common dimensions.
        </p><p>
        Examples:
        </p>
<pre>{% raw %}
join(t1, t2, f(x,y)(x * y)) 
{% endraw %}</pre>
<a href="https://docs.vespa.ai/playground/#N4KABGBEBmkFxgNrgmUrWQPYAd5QFNIAaFDSPBdDTAO30gEcBXAgJwE8AKAFwEoSZGpCIJIPArQDOWNgB5oAGywBDHgD4uAD2ABfPnGDAtcAAy6EARgB0p4mhOWLYAEy3dkIRF1DdpDNTkuAxE-sKUaF50DGo8bACWAEbMErwCYTSEDBLSsgrKapo6fhx6BkYmdhxmzgDMtvbGZsTVTggALA0OcJYtNQgArF1Nva3OAGzunpk+GH5CgZjBYqFRFPiL5PRiAFZY8fQZwqJQewdcLOzc-PaxCcmpN2DQ2i182mAAVGAcfAJRs1QgIAuiBdEA">
  playground example join
</a>
    </td>
</tr>
<tr>
    <th>merge(<br/>&nbsp;&nbsp;tensor1,<br/>&nbsp;&nbsp;tensor2,<br/>&nbsp;&nbsp;f(x,y)(expr)<br/>)</th>
    <td>
        <p id="merge">
        Returns a new tensor consisting of all cells from both the arguments, where the lambda function is used
        to produce a single value in the cases where both arguments provide a value for a cell.
        </p><p>
        Arguments:
        </p>
        <ul>
          <li><code>tensor1</code>: a tensor expression.</li>
          <li><code>tensor2</code>: a tensor expression.</li>
          <li><code>f(x,y)(expr)</code>: a <a href="#lambda">lambda function</a> with two arguments.</li>
        </ul>
        <p>
        Returns a new tensor having all the cells of both arguments,
        where the lambda is invoked to produce a single value only when both arguments have a value for the same cell.
        </p><p>
        The argument tensors must have the same type, and that will be the type of the resulting tensor. Example:
        </p>
<pre>{% raw %}
merge(t1, t2, f(left,right)(right))
{% endraw %}</pre>
<a href="https://docs.vespa.ai/playground/#N4KABGBEBmkFxgNrgmUrWQPYAd5QFNIAaFDSPBdDTAO30gEcBXAgJwE8AKAFwEoSZGpCIJIPArQDOWNlwDWBDsAC+xAB6IATAF0+cYAEM4iAIzFdxAEYmAzMQAsOlZCEQVQtUOrlcDIqQ0UJRobnQMhjw8bACWVswSvAKBQYQMEtKyCkqqGtp6BjaIAKzEAGw6xADGJgDsxAAczq5BHhheGD6YfmIBYRT4XeT0YgC27ADmBAD6BOqGozgANn2paWOTBFws7Nz8xGCR0XEJW-tg0Fwr0DzEsRMAFvxc9098AmFtqF86ICpAA">
  playground example merge
</a>
    </td>
</tr>
<tr>
    <th>tensor(<br/>&nbsp;&nbsp;tensor-type-spec<br/>)(expr)</th>
    <td>
        <p id="tensor">
        Generates new tensors according to type specification and expression <code>expr</code>.
        </p><p>
        Arguments:
        </p>
        <ul>
          <li><code>tensor-type-spec</code>: an <a href="tensor.html#tensor-type-spec">indexed tensor type specification.</a></li>
          <li><code>(expression)</code>: a <a href="#lambda">lambda function</a> expressing how to generate the tensor.</li>
        </ul>
        <p>
        Generates new tensors according to the type specification and expression <code>expr</code>.
        The tensor type must be an indexed tensor (e.g. <code>tensor&lt;float&gt;(x[10])</code>).
        The expression in <code>expr</code> will be evaluated for each cell.
        The arguments in the expression is implicitly the names of the dimensions defined in the type spec.
        </p><p>
        Useful for creating transformation tensors. Examples:
        </p>
<pre>{% raw %}
tensor&lt;float&gt;(x[3])(x)
{% endraw %}</pre>
<a href="https://docs.vespa.ai/playground/#N4KABGBEBmkFxgNrgmUrWQPYAd5QFNIAaFDSPBdDTAO30gHMDaCAnAQwBcCB9VgO4kyNSEQSQetAM5Y2ACgAeiAMwBdAJRKNkERAC+I-aQzVyuBkROjKaPXQbNWnHvwIDeAJmE1M4qFKybAA80AA2WNwAfEqInmrEAJ5xmkpgALzpYIk69oYYxiJmmBYSVvYU+MXk9BJcPr6EDIFysQAsmnCIAIwAdAAMxGCeA0Mqo2BtA2q6vvmohaYVpU3W5LbVDhJO7Nx8grzQbFgAtrwctFhcABbsvC1sDb5izSxB8tApWlzAinDQAGpuvpcnMjCg1CB9EA">
  playground generate examples
</a>
    </td>
</tr>
<tr>
    <th>rename(<br/>&nbsp;&nbsp;tensor,<br/>&nbsp;&nbsp;dim-to-rename,<br/>&nbsp;&nbsp;new-names<br/>)</th>
    <td>
        <p id="rename">
        Renames one or more dimensions in the tensor.
        </p><p>
        Arguments:
        </p>
        <ul>
          <li><code>tensor</code>: a tensor expression.</li>
          <li><code>dim-to-rename</code>: a dimension, or list of dimensions, to rename.</li>
          <li><code>new-names</code>: new names for the dimensions listed above.</li>
        </ul>
        Returns a new tensor with one or more dimension renamed. Examples:
<pre>{% raw %}
rename(t1,x,z)
{% endraw %}</pre>
<a href="https://docs.vespa.ai/playground/#N4KABGBEBmkFxgNrgmUrWQPYAd5QFNIAaFDSPBdDTAO30gBcSybIiEmDaBnLAJwAUAD2ABfYgE9xASjjBgwuAAYpKsQgCMAOlVolqyXE0awy3cX3G1y0+b2LrRk1t1jIrCGNYTW1crgMRKQ0UJRonnQM-NwAhgC2BAD6BMIJOAA2wZEQ7NFxiYKMxMLEAF4yHqHeGL4Y-piBnNmhFPgN5PScMbQJyanpWUmMAO5YLKG5HFA9fUXEIlIyCwCWxABWMpWRNai7ALogYkA">
  playground rename examples
</a>
    </td>
</tr>
<tr>
    <th>concat(<br/>&nbsp;&nbsp;tensor1,<br/>&nbsp;&nbsp;tensor2,<br/>&nbsp;&nbsp;dim<br/>)</th>
    <td>
        <p id="concat">
        Concatenates two tensors along dimension <code>dim</code>.
        </p><p>
        Arguments:
        </p>
        <ul>
          <li><code>tensor1</code>: a tensor expression.</li>
          <li><code>tensor2</code>: a tensor expression.</li>
          <li><code>dim</code>: the dimension to concatenate along.</li>
        </ul>
        Returns a new tensor with the two tensors <code>tensor1</code> and
        <code>tensor2</code> concatenated along dimension <code>dim</code>. Examples:
<pre>{% raw %}
concat(t,t2,x)
{% endraw %}</pre>
<a href="https://docs.vespa.ai/playground/#N4KABGBEBmkFxgNrgmUrWQPYAd5QFNIAaFDSPBdDTAO30gBcSybIiEmDaBnLAJwAUAD0QBmALoBKOIgAsxAKzEAbBMisIAX1ZbSGauVwMi+tpTSa6DRgCYWNTByiNufIaNvTZY4nPVWOhh6rIaYxpymVhT4YeT0nADGWLSJAIbMZo7sDMmpGYKMxHbEwlIajkGoIQbREYQO5rFWEJAJUHnpjAD6WIwAFgT83QDuaQCejdnOkJ0FJUVlFTRV2igSIFpAA">
  playground concat examples
</a>
    </td>
</tr>
<tr>
    <th style="white-space:nowrap;">(tensor)partial-address</th>
    <td>
        <p id="slice">
        Slice - returns a new tensor containing the cells matching the partial address.
        </p><p>
        Arguments:
        </p>
        <ul>
          <li><code>tensor</code>: a tensor expression.</li>
          <li><code>partial-address</code>: Can be given in the form of a tensor address <code>{dimension:label,..}</code>,
            or for tensors referenced directly and having a single mapped or indexed type respectively
            as just a label in curly brackets <code>{label}</code>
            or just an index in square brackets, <code>[index]</code>.
            Index labels may be specified by a lambda expression enclosed in parentheses.</li>
        </ul>
        Returns a new tensor containing the cells matching the partial address.
        A common special case is producing a single value by specifying a full address.
        The type of the resulting tensor is the dimensions of the argument tensor not specified by the partial address. Examples:
<pre>{% raw %}
# a_tensor is of type tensor(key{},x[2])
a_tensor{key:key1,x:1}
{% endraw %}</pre>
<a href="https://docs.vespa.ai/playground/#N4KABGBEBmkFxgNrgmUrWQPYAd5QFNIAaFDSPBdDTAO30gBcSybIiEmDaBnLAJwAUAD0QBmALoBKOIgAsxAKzEAbBMisIAX1ZbSGauVwMi+tpTSa6DHgBsAlgGNTViOwaNgwuAAYtGmjAdDD1WQ0xjThdAinxw8npOOycCAH0AJhZAtw4oT290-ytg1FCDK2wLdzNyC3jrTkZMmrZcrl4BQQBrAgBPYD1RdOk4YDAe3oBGWUmAOh9iMHT5iUWJ9NkxecW5FaCAmhLtFvrKkyzzONcoRKhk51SAWwBDHBwCABNU+1oPgmFPhcYm0msAJnAJpNiN5JkVAkcgicKpFCEDMHVrpBbpB7mkXm9Pt9fv9Cc1MSD0mC+hC+lDvIIxABadJSOGHXQoCQgLRAA">
  playground slice examples
</a>
    </td>
</tr>
<tr>
    <th style="white-space:nowrap;">tensor-literal-form</th>
    <td>
        <p id="literal">
        Returns a new tensor having the type and cell values given explicitly.
        Each cell value may be supplied by a lambda which can access other features.
        </p><p>
        Returns a new tensor from the <a href="tensor.html#tensor-literal-form">literal form</a>,
        where the type must be specified explicitly.
        Each value may be supplied by a lambda,
        which - in contrast to all other lambdas -
        <em>may refer to features and expressions from the context</em>. Examples:
        </p>
<pre>{% raw %}
# Declare an indexed tensor 
tensor(x[2]):[1.0, 2.0]]

# Declare an mapped tensor
tensor(x{}):{x1:3, x2:4} 
{% endraw %}</pre>
    </td>
</tr>
<tr>
    <th>cell_cast(<br/>&nbsp;&nbsp;tensor,<br/>&nbsp;&nbsp;cell_type<br/>)</th>
    <td>
        <p id="cell_cast">
        Returns a new tensor that is the same as the argument, except
        that all cell values are converted to the given
        <a href="tensor.html#tensor-type-spec">cell type</a>.
        </p><p>
        Arguments:
        </p>
        <ul>
          <li><code>tensor</code>: a tensor expression.</li>
          <li><code>cell_type</code>: wanted cell type.</li>
        </ul>
	      Example, casting from <code>bfloat16</code> to <code>float</code>:
<pre>{% raw %}
# With a tensor t of the type tensor&lt;bfloat16&gt;(x[5])(x+1)
cell_cast(t, float)
{% endraw %}</pre>
    </td>
</tr>
</tbody>
</table>


<h3 id="lambda">Lambda functions in primitive functions</h3>
<p>
Some of the primitive functions accept lambda functions that are evaluated
and applied to a set of tensor cells.
The functions contain a single expression that have the same format and built-in functions as
<a href="ranking-expressions.html">general ranking expressions</a>.
However, the atoms are the arguments defined in the argument list of the lambda.
</p><p>
The expression cannot access variables or data structures outside of the lambda,
i.e. they are not closures.
</p><p>
Examples:
</p>
<pre>
f(x)(log(x))
f(x,y)(if(x &lt; y, 0, 1))
</pre>



<h3 id="non-primitive-functions">Non-primitive functions</h3>
<p>
Non-primitive functions can be implemented by primitive functions,
but are not necessarily so for performance reasons.
</p>
<table class="table">
  <thead>
  <tr>
    <th>Function</th>
    <th>Description</th>
  </tr>
  </thead>
  <tbody>
  <tr>
    <th>acos(t)</th>
    <td>
      <p id="acos-t"><code>map(t, f(x)(acos(x)))</code></p>
      <p>Arc cosine of all elements.</p>
    </td>
  </tr>
  <tr>
    <th>t1 + t2 (add)</th>
    <td>
      <p id="t1-t2-add"><code>join(t1, t2, f(x,y)(x + y))</code></p>
      <p>Join and sum tensors <code>t1</code> and <code>t2</code>.</p>
    </td>
  </tr>
  <tr>
    <th>argmax(t, dim)</th>
    <td>
      <p id="argmax"><code>join(t, reduce(t, max, dim), f(x,y)(if (x == y, 1, 0)))</code></p>
      <p>
        Returns a tensor with cell(s) of the highest value(s) in the tensor set to 1.
        The dimension argument follows the same format as reduce as multiple dimensions can be given and is optional.
      </p>
    </td>
  </tr>
  <tr>
    <th>argmin(t, dim)</th>
    <td>
      <p id="argmin"><code>join(t, reduce(t, min, dim), f(x,y)(if (x == y, 1, 0)))</code></p>
      <p>
        Returns a tensor with cell(s) of the lowest value(s) in the tensor set to 1.
        The dimension argument follows the same format as reduce as multiple dimensions can be given and is optional.
      </p>
    </td>
  </tr>
  <tr>
    <th>asin(t)</th>
    <td>
      <p id="asin-t"><code>map(t, f(x)(asin(x)))</code></p>
      <p>Arc sine of all elements.</p>
    </td>
  </tr>
  <tr>
    <th>atan(t)</th>
    <td>
      <p id="atan-t"><code>map(t, f(x)(atan(x)))</code></p>
      <p>Arc tangent of all elements.</p>
    </td>
  </tr>
  <tr>
    <th>atan2(t1, t2)</th>
    <td>
      <p id="atan2-t1-t2"><code>join(t1, t2, f(x,y)(atan2(x,y)))</code></p>
      <p>Arctangent of <code>t1</code> and <code>t2</code>/</p>
    </td>
  </tr>
  <tr>
    <th>avg(t, dim)</th>
    <td>
      <p id="avg-t"><code>reduce(t, avg, dim)</code></p>
      <p>
        Reduce the tensor with the <code>average</code> aggregator along dimension <code>dim</code>.
        If the dimension argument is omitted, this reduces over all dimensions.
      </p>
    </td>
  </tr>
  <tr>
    <th>ceil(t)</th>
    <td>
      <p id="ceil-t"><code>map(t, f(x)(ceil(x)))</code></p>
      <p>Ceiling of all elements.</p>
    </td>
  </tr>
  <tr>
    <th>count(t, dim)</th>
    <td>
      <p id="count-t"><code>reduce(t, count, dim)</code></p>
      <p>
        Reduce the tensor with the <code>count</code> aggregator along dimension <code>dim</code>.
        If the dimension argument is omitted, this reduces over all dimensions.
      </p>

    </td>
  </tr>
  <tr>
    <th>cos(t)</th>
    <td>
      <p id="cos-t"><code>map(t, f(x)(cos(x)))</code></p>
      <p>Cosine of all elements.</p>
    </td>
  </tr>
  <tr>
    <th>cosh(t)</th>
    <td>
      <p id="cosh-t"><code>map(t, f(x)(cosh(x)))</code></p>
      <p>Hyperbolic cosine of all elements.</p>
    </td>
  </tr>
  <tr>
    <th>cosine_similarity(t1, t2, dim)</th>
    <td>
      <p id="cosine-similarity-t"><code>reduce(t1*t2, sum, dim) / sqrt(reduce(t1*t1, sum, dim) * reduce(t2*t2, sum, dim))</code></p>
      <p>The cosine similarity between the two vectors in the given dimension.</p>
    </td>
  </tr>
  <tr>
    <th>diag(n1, n2)</th>
    <td>
      <p id="diag-n1-n2"><code>tensor(i[n1],j[n2])(if (i==j, 1.0, 0.0)))</code></p>
      <p>Returns a tensor with the diagonal set to 1.0.</p>
    </td>
  </tr>
  <tr>
    <th>t1 / t2 (div)</th>
    <td>
      <p id="t1-t2-dov"><code>join(t1, t2, f(x,y)(x / y))</code></p>
      <p>Join and divide tensors <code>t1</code> and <code>t2</code>.</p>
    </td>
  </tr>
  <tr>
    <th>elu(t)</th>
    <td>
      <p id="elu-t"><code>map(t, f(x)(if(x &lt; 0, exp(x)-1, x)))</code></p>
      <p><a href="https://arxiv.org/abs/1511.07289">Exponential linear unit</a>.</p>
    </td>
  </tr>
  <tr>
    <th>t1 == t2 (equal)</th>
    <td>
      <p id="t1-t2-equal"><code>join(t1, t2, f(x,y)(x == y))</code></p>
      <p>Join and determine if each element in <code>t1</code> and <code>t2</code> are equal.</p>
    </td>
  </tr>
  <tr>
    <th>euclidean_distance(t1, t2, dim)</th>
    <td>
      <p id="euclidean-distance-t"><code>join(reduce(map(join(t1, t2, f(x,y)(x-y)), f(x)(x * x)), sum, dim), f(x)(sqrt(x)))</code></p>
      <p>euclidean_distance: <code>sqrt(sum((t1-t2)^2, dim))</code>.</p>
    </td>
  </tr>
  <tr>
    <th>exp(t)</th>
    <td>
      <p id="exp-t"><code>map(t, f(x)(exp(x)))</code></p>
      <p>Exponential function (e^x) of each element.</p>
    </td>
  </tr>
  <tr>
    <th>expand(t, dim)</th>
    <td>
      <p id="expand-t"><code>t * tensor(dim[1])(1)</code></p>
      <p>Adds an indexed dimension with name <code>dim</code> to the tensor <code>t</code>.</p>
    </td>
  </tr>
  <tr>
    <th>floor(t)</th>
    <td>
      <p id="floor-t"><code>map(t, f(x)(floor(x)))</code></p>
      <p>Floor of each element.</p>
    </td>
  </tr>
  <tr>
    <th>t1 &gt; t2 (greater)</th>
    <td>
      <p id="t1-t2-greater"><code>join(t1, t2, f(x,y)(x &gt; y))</code></p>
      <p>Join and determine if each element in <code>t1</code> is greater than <code>t2</code>.</p>
    </td>
  </tr>
  <tr>
    <th style="white-space: nowrap">t1 &gt;= t2 (greater or equals)</th>
    <td>
      <p id="t1-t2-greater-or-equals"><code>join(t1, t2, f(x,y)(x &gt;= y))</code></p>
      <p>Join and determine if each element in <code>t1</code> is greater than or equals <code>t2</code>.</p>
    </td>
  </tr>
  <tr>
    <th>t1 &lt; t2 (less)</th>
    <td>
      <p id="t1-t2-less"><code>join(t1, t2, f(x,y)(x &lt; y))</code></p>
      <p>Join and determine if each element in <code>t1</code> is less than <code>t2</code>.</p>
    </td>
  </tr>
  <tr>
    <th>t1 &lt;= t2 (less or equals)</th>
    <td>
      <p id="t1-t2-less-or-equals"><code>join(t1, t2, f(x,y)(x &lt;= y))</code></p>
      <p>Join and determine if each element in <code>t1</code> is less than or equals <code>t2</code>.</p>
    </td>
  </tr>
  <tr>
    <th>l1_normalize(t, dim)</th>
    <td>
      <p id="l1-normalize-t"><code>join(t, reduce(t, sum, dim), f(x,y) (x / y))</code></p>
      <p>L1 normalization: <code>t / sum(t, dim)</code>.</p>
    </td>
  </tr>
  <tr>
    <th>l2_normalize(t, dim)</th>
    <td>
      <p id="l2-normalize-t"><code>join(t, map(reduce(map(t, f(x)(x * x)), sum, dim), f(x)(sqrt(x))), f(x,y)(x / y))</code></p>
      <p>L2 normalization: <code>t / sqrt(sum(t^2, dim)</code>.</p>
    </td>
  </tr>
  <tr>
    <th>log(t)</th>
    <td>
      <p id="log-t"><code>map(t, f(x)(log(x)))</code></p>
      <p>Natural logarithm of each element.</p>
    </td>
  </tr>
  <tr>
    <th>log10(t)</th>
    <td>
      <p id="log10-t"><code>map(t, f(x)(log10(x)))</code></p>
      <p>Logarithm with base 10 of each element.</p>
    </td>
  </tr>
  <tr>
    <th>matmul(t1, t2, dim)</th>
    <td>
      <p id="matmul-t1-t2"><code>reduce(join(t1, t2, f(x,y)(x * y)), sum, dim)</code></p>
      <p>
        Matrix multiplication of two tensors.
        This is the product of the two tensors summed along a shared dimension.
      </p>
    </td>
  </tr>
  <tr>
    <th>max(t, dim)</th>
    <td>
      <p id="max-t"><code>reduce(t, max, dim)</code></p>
      <p>Reduce the tensor with the <code>max</code> aggregator along dimension <code>dim</code>.</p>
    </td>
  </tr>
  <tr>
    <th>max(t1, t2)</th>
    <td>
      <p id="max-t1-t2"><code>join(t1, t2, f(x,y)(max(x,y)))</code></p>
      <p>
        Join and return the max of <code>t1</code> or <code>t2</code>.
        Arguments can be scalars.
      </p>
    </td>
  </tr>
  <tr>
    <th>hamming(t1, t2)</th>
    <td>
      <p id="hamming-t1-t2"><code>join(t1, t2, f(x,y)(hamming(x,y)))</code></p>
      <p>
        Join and return the Hamming distance between matching cells of <code>t1</code> and <code>t2</code>.
        This function is mostly useful when the input contains vectors with binary data
        and summing the hamming distance over the vector dimension, e.g.:
      </p>
      <table class="table">
        <thead></thead><tbody>
      <tr>
        <td>type of input <em>t1</em> &rarr;</td><td>
        <code>tensor&lt;int8&gt;(dimone{},z[32])</code>
      </td>
      </tr><tr>
        <td>type of input <em>t2</em> &rarr;</td><td>
        <code>tensor&lt;int8&gt;(dimtwo{},z[32])</code>
      </td>
      </tr><tr>
        <td>expression &rarr;</td><td>
        <code>reduce(join(t1, t2, f(a,b)(hamming(a,b)), sum, z)</code>
      </td>
      </tr><tr>
        <td>output type &rarr;</td><td>
        <code>tensor&lt;float&gt;(dimone{},dimtwo{})</code>
      </td>
      </tr>
      </tbody>
      </table>
      <p>
        Note that the cell values are always treated as if they were both 8-bit integers in the range [-128,127],
        and only then counting the number of bits that are different.
        See also the corresponding <a href="schema-reference.html#distance-metric">distance metric</a>.
        Arguments can be scalars.
      </p>
    </td>
  </tr>
  <tr>
    <th>median(t, dim)</th>
    <td>
      <p id="median-t"><code>reduce(t, median, dim)</code></p>
      <p>
        Reduce the tensor with the <code>median</code> aggregator along dimension <code>dim</code>.
        If the dimension argument is omitted, this reduces over all dimensions.
      </p>
    </td>
  </tr>
  <tr>
    <th>min(t, dim)</th>
    <td>
      <p id="min-t"><code>reduce(t, min, dim)</code></p>
      <p>Reduce the tensor with the <code>min</code> aggregator along dimension <code>dim</code>.</p>
    </td>
  </tr>
  <tr>
    <th>min(t1, t2)</th>
    <td>
      <p id="min-t1-t2"><code>join(t1, t2, f(x,y)(min(x,y)))</code></p>
      <p>
        Join and return the minimum of <code>t1</code> or <code>t2</code>.
        Arguments can be scalars.
      </p>
    </td>
  </tr>
  <tr>
    <th>mod(t, constant)</th>
    <td>
      <p id="mod-t"><code>map(t, f(x)(mod(x,constant)))</code></p>
      <p>Modulus of <code>constant</code> with each element.</p>
    </td>
  </tr>
  <tr>
    <th>t1 * t2 (mul)</th>
    <td>
      <p id="t1-t2-mul"><code>join(t1, t2, f(x,y)(x * y))</code></p>
      <p>Join and multiply tensors <code>t1</code> and <code>t2</code>.</p>
    </td>
  </tr>
  <tr>
    <th>t1 != t2 (not equal)</th>
    <td>
      <p id="t1-t2-not-equal"><code>join(t1, t2, f(x,y)(x != y))</code></p>
      <p>Join and determine if each element in <code>t1</code> and <code>t2</code> are not equal.</p>
    </td>
  </tr>
  <tr>
    <th>pow(t, constant)</th>
    <td>
      <p id="pow-t"><code>map(t, f(x)(pow(x,constant)))</code></p>
      <p>Raise each element to the power of <code>constant</code>.</p>
    </td>
  </tr>
  <tr>
    <th>prod(t, dim)</th>
    <td>
      <p id="prod-t"><code>reduce(t, prod, dim)</code></p>
      <p>
        Reduce the tensor with the <code>product</code> aggregator along dimension <code>dim</code>.
        If the dimension argument is omitted, this reduces over all dimensions.
      </p>
    </td>
  </tr>
  <tr>
    <th>random(n1, n2, ...)</th>
    <td>
      <p id="random-n1-n2"><code>tensor(i1[n1],i2[n2],...)(random(1.0))</code></p>
      <p>Returns a tensor with random values between 0.0 and 1.0, uniform distribution.</p>
    </td>
  </tr>
  <tr>
    <th>range(n)</th>
    <td>
      <p id="range-t"><code>tensor(i[n])(i)</code></p>
      <p>Returns a tensor with increasing values.</p>
    </td>
  </tr>
  <tr>
    <th>relu(t)</th>
    <td>
      <p id="relu-t"><code>map(t, f(x)(max(0,x)))</code></p>
      <p>Rectified linear unit.</p>
    </td>
  </tr>
  <tr>
    <th>sigmoid(t)</th>
    <td>
      <p id="sigmoid-t"><code>map(t, f(x)(1.0 / (1.0 + exp(0.0-x))))</code></p>
      <p>Returns the sigmoid of each element.</p>
    </td>
  </tr>
  <tr>
    <th>sin(t)</th>
    <td>
      <p id="sin-t"><code>map(t, f(x)(sin(x)))</code></p>
      <p>Sinus of each element.</p>
    </td>
  </tr>
  <tr>
    <th>sinh(t)</th>
    <td>
      <p id="sinh-t"><code>map(t, f(x)(sinh(x)))</code></p>
      <p>Hyperbolic sinus of each element.</p>
    </td>
  </tr>
  <tr>
    <th>softmax(t, dim)</th>
    <td>
      <p id="softmax-t"><code>join(map(t, f(x)(exp(x))), reduce(map(t, f(x)(exp(x))), sum, dim), f(x,y)(x / y))</code></p>
      <p>The softmax of the tensor, e.g. <code>e^x / sum(e^x)</code>.</p>
    </td>
  </tr>
  <tr>
    <th>sqrt(t)</th>
    <td>
      <p id="sqrt-t"><code>map(t, f(x)(sqrt(x)))</code></p>
      <p>The square root of each element.</p>
    </td>
  </tr>
  <tr>
    <th>t1 - t2 (subtract)</th>
    <td>
      <p id="t1-t2-subtract"><code>join(t1, t2, f(x,y)(x - y))</code></p>
      <p>Join and subtract tensors <code>t1</code> and <code>t2</code>.</p>
    </td>
  </tr>
  <tr>
    <th>sum(t, dim)</th>
    <td>
      <p id="sum-t"><code>reduce(t, sum, dim)</code></p>
      <p>
        Reduce the tensor with the <code>summation</code> aggregator along dimension <code>dim</code>.
        If the dimension argument is omitted, this reduces over all dimensions.
      </p>
    </td>
  </tr>
  <tr>
    <th>tan(t)</th>
    <td>
      <p id="tan-t"><code>map(t, f(x)(tan(x)))</code></p>
      <p>The tangent of each element.</p>
    </td>
  </tr>
  <tr>
    <th>tanh(t)</th>
    <td>
      <p id="tanh-t"><code>map(t, f(x)(tanh(x)))</code></p>
      <p>The hyperbolic tangent of each element.</p>
    </td>
  </tr>
  <tr>
    <th>unpack_bits(t)</th>
    <td>
      <p id="unpack-bits">unpacks bits from int8 input to 8 times as many floats</p>
      <p>The innermost indexed dimension will expand to have 8 times as many cells,
         each with a float value of either 0.0 or 1.0 determined by one bit in
         the 8-bit input value.  Comparable to <code>numpy.unpackbits</code>
         which gives the same basic functionality.
         A minimal input such as <code>tensor&lt;int8&gt;(x[1]):[9]</code>
         would give output <code>tensor&lt;float&gt;(x[8]):[0,0,0,0,1,0,0,1]</code>
         (default bit-order is big-endian).
         As a very complex example, an input with type <code>tensor&lt;int8&gt;(foo{},x[3],y[11],z{})</code>
         will produce output with type <code>tensor&lt;float&gt;(foo{},x[3],y[88],z{})</code>
         where "foo", "x" and "z" are unchanged, as "y" is the innermost indexed dimension.
      </p>
    </td>
  </tr>
  <tr>
    <th>unpack_bits(t, cell_type)</th>
    <td>
      <p id="unpack-bits-cell-type">unpacks bits from int8 input to 8 times as many values</p>
      <p>Same as above, but with optionally different cell_type (could be <code>double</code>
         for example, if you will combine the output with other tensors using
         double).
      </p>
    </td>
  </tr>
  <tr>
    <th>unpack_bits(t, cell_type, endian)</th>
    <td>
      <p id="unpack-bits-cell-type-endian">unpacks bits from int8 input to 8 times as many values</p>
      <p>Same as above, but also optionally different endian for the bits; must be
         either <code>big</code> (default) or <code>little</code>.
      </p>
    </td>
  </tr>
  <tr>
    <th>xw_plus_b(x, w, b, dim)</th>
    <td>
      <p id="xw-plus-b"><code>join(reduce(join(x, w, f(x,y)(x * y)), sum, dim), b, f(x,y)(x+y))</code></p>
      <p>
        Matrix multiplication of <code>x</code> (usually a vector) and <code>w</code> (weights),
        with <code>b</code> added (bias).
        A typical operation for activations in a neural network layer,
        e.g. <code>sigmoid(xw_plus_b(x,w,b)))</code>.
      </p>
    </td>
  </tr>
  </tbody>
</table>
